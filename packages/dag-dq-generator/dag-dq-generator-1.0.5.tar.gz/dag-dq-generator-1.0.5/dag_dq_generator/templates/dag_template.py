# 
# This file was programmatically generated by the DAG GENERATOR at https://git.corp.adobe.com/ccea/dag-dq-generator
#
# PLEASE DO NOT MODIFY THE FILE
# 

# General Imports
{% from '_global_macros.py' import create_group -%}
import os
from datetime import datetime, timedelta
from dateutil import tz
from typing import List
import logging

# Airflow Imports
from airflow import DAG, settings
from airflow.providers.databricks.hooks.databricks_sql import DatabricksSqlHook
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator, DatabricksRunNowOperator
from airflow.providers.databricks.operators.databricks_sql import DatabricksSqlOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.tableau.operators.tableau import TableauOperator
from airflow.providers.tableau.sensors.tableau import TableauJobStatusSensor
from airflow.providers.http.sensors.http import HttpSensor
from airflow.models import Variable, TaskInstance, XCom
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup # Used to group tasks together in the Graph view of the Airflow UI
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.email import EmailOperator
from airflow.utils.email import send_email
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.sensors.date_time import DateTimeSensor
from airflow.utils.edgemodifier import Label # Used to label node edges in the Airflow UI
from airflow.decorators import dag, task # DAG and task decorators for interfacing with the TaskFlow API
from airflow.models.baseoperator import chain # A function that sets sequential dependencies between tasks including lists of tasks
from airflow.utils.trigger_rule import TriggerRule # Used to change how an Operator is triggered
from airflow.utils.state import State

# Define global constants
DAG_NAME = "{{pipeline_settings.dag_name}}"
DAG_OWNER = "{{pipeline_settings.dag_owner}}"
{% if pipeline_settings.sql_endpoint_name -%}
DATABRICKS_SQL_ENDPOINT_NAME = "{{pipeline_settings.sql_endpoint_name}}"
{%- endif -%}
{% if pipeline_settings.sql_endpoint_http_path -%}
DATABRICKS_SQL_ENDPOINT_HTTP_PATH = "{{pipeline_settings.sql_endpoint_http_path}}"
{%- endif -%}
DATABRICKS_CONN_ID = "{{pipeline_settings.conn_id}}"

{% if pipeline_settings.clusters -%}
# Instantiate Databricks cluster settings if any
{% for cluster in pipeline_settings.clusters -%}
{{cluster.name}} = {{cluster.params}}
{% endfor %}
{%- endif %}
{% if pipeline_settings.operator_default_args -%}
# Define arguments w/ extra arguments if necessary
extra_operator_args = {{pipeline_settings.operator_default_args}}
{%- endif %}

# get the airflow.task logger
task_logger = logging.getLogger('airflow.task')

default_args = {
    "owner": DAG_OWNER,
    "databricks_conn_id": DATABRICKS_CONN_ID,
    {% if pipeline_settings.sql_endpoint_name %}
    "sql_endpoint_name": DATABRICKS_SQL_ENDPOINT_NAME,
    {% endif %}
    {% if pipeline_settings.sql_endpoint_http_path %}
    "sql_endpoint_http_path": DATABRICKS_SQL_ENDPOINT_HTTP_PATH,
    {% endif %}
    "do_xcom_push": True,
    {% if pipeline_settings.retry_delay %}
    "retry_delay": {{pipeline_settings.retry_delay}},
    {% endif %}
    {% if pipeline_settings.operator_default_args %}
    **extra_operator_args,
    {% endif %}
}
{% if pipeline_settings.dag_args -%}
dag_args = {{pipeline_settings.dag_args}}
{%- endif %}

# Define Python Operator callable functions
def _task_on_success_callback(context):
    ti = context["ti"]
    dag_id = ti.dag_id
    task_id = ti.task_id

    task_logger.debug(f"setting _stats as 'task_id': {task_id}, 'duration': {ti.duration}")
    ti.xcom_push(key="_stats", value={'task_id': task_id, 'duration': ti.duration})

    {% if pipeline_settings.success_email_list and pipeline_settings.send_start_email %}
    if task_id == 'begin':
        msg = f"""
            <style>
                body {{'{{ 
                font-family: "Adobe Clean", Ubuntu, sans-serif; 
                font-size: 0.94em;
                }}'}}
            </style>
            <body>
                Hi,<br><br><i>{dag_id}</i> DAG started. Please check the DAG run logs for more details.
                <br>
            """
        msg += "<br>Sincerely, <br> {{pipeline_settings.team_name|default('CI Data Engineering')}}</body>"
        subject = f"[STARTED] {dag_id}"
        send_email(
            to={{pipeline_settings.success_email_list}}, subject=subject, html_content=msg
        )
    {% endif %}
    

def on_success_callback(context):
    ti = context["ti"]
    dag_id = ti.dag_id
    task_id = ti.task_id
    {% if pipeline_settings.success_email_list %}
    session = settings.Session()
    tasks = {}
    for task in session.query(TaskInstance).filter(TaskInstance.dag_id == dag_id, TaskInstance.run_id == ti.run_id).order_by(TaskInstance.start_date).all():
        tasks[task.task_id] = task.start_date
    
    dag_start_date = tasks['begin']
    
    task_logger.info(f"Session tasks - {session.query(TaskInstance).filter(TaskInstance.dag_id == dag_id, TaskInstance.run_id == ti.run_id).all()}")

    stats = ti.xcom_pull(key="_stats", task_ids=list(tasks.keys()))
    run_params = [(_xcom.key, _xcom.value) for _xcom in XCom.get_many(run_id=ti.run_id, dag_ids=dag_id, task_ids='begin', session=session).all() if _xcom.key != '_stats']

    task_logger.info(f"DAG run params are {run_params}")

    dag_duration = (datetime.now(dag_start_date.tzinfo) - dag_start_date).seconds

    msg = f"""
        <style>
            body {{'{{ 
            font-family: "Adobe Clean", Ubuntu, sans-serif; 
            font-size: 0.94em;
            }}'}}
        </style>
        <body>
            Hi,<br><br><i>{dag_id}</i> DAG succeeded with the following run stats: <br><br>
            &emsp;&emsp;- Start time: <b>{dag_start_date.astimezone(tz.gettz('US/Pacific')).strftime('%Y-%m-%d %H:%M:%S')} PST</b><br>
            &emsp;&emsp;- Total duration: <b>{(dag_duration/60):0.2f}</b> mins<br>
            <br>
            &emsp;&emsp;<u>Task-level Durations</u><br>
        """
    _pos = -1; _start = datetime.now(dag_start_date.tzinfo)
    for stat in stats:
        task_start_date = tasks[stat['task_id']].astimezone(tz.gettz('US/Pacific'))
        if (task_start_date - _start).seconds > 5: # Do not start within 5 seconds of each other
            _pos += 1
            _start = task_start_date
        msg += f"&emsp;&emsp;-{'&emsp;&emsp;&emsp;&emsp;' * _pos} {stat['task_id']}: start time: {task_start_date.strftime('%Y-%m-%d %H:%M:%S')} PST / duration: <b>{(stat['duration']/60):.2f}</b> mins<br>"
 
    if len(run_params) > 0:
        msg += "<br>&emsp;&emsp;<u>Run Variables</u><br>"
        for param in run_params:
            msg += f"&emsp;&emsp;- {param[0]}: <b>{param[1]}</b><br>"

    msg += "<br>Sincerely, <br> {{pipeline_settings.team_name|default('CI Data Engineering')}}</body>"
    subject = f"[SUCCESS] {dag_id}"
    send_email(
        to={{pipeline_settings.success_email_list}}, subject=subject, html_content=msg
    )
    {% endif %}

def _verify_check_response(**kwargs) -> None:
    """Pulls and returns DatabricksSqlOperator data from XCom"""
    try:
        ti = kwargs["ti"]
        dag_id = ti.dag_id
        task_id = ti.task_id
        pulled_value = ti.xcom_pull(key='return_value', task_ids=kwargs["sql_task_id"])

        if pulled_value is None or len(pulled_value) < 1:
            return 1

        {% if pipeline_settings.success_email_list %}
        if not pulled_value[0][0]:
            msg = f"""
                <style>
                    body {{'{{ 
                    font-family: "Adobe Clean", Ubuntu, sans-serif; 
                    font-size: 0.94em;
                    }}'}}
                </style>
                <body>
                    Hi,<br><br><i>{dag_id} - {task_id}</i> check failed. Skipping downstream tasks. Please check why the verification condition wasn't met.<br><br>
                    {kwargs['sql_task_args']['sql']}
                    <br>
                """
            msg += "<br>Sincerely, <br> {{pipeline_settings.team_name|default('CI Data Engineering')}}</body>"
            subject = f"[SKIPPED] {dag_id} - {task_id}"
            send_email(to=ti.task.email, subject=subject, html_content=msg)
        {% endif %}

        return pulled_value[0][0]
    
    except Exception as e:
        task_logger.error(e)
        return 0

{% if sensors -%}

def _check_httpsensor_dag_response(response):
    upstream_dag_id = response.request.url.split("/")[-2]
    state = "failed"
    try:
        response_json = response.json()
        task_logger.info(f"Response JSON: {response_json}")
        state = response_json["dag_runs"][0]["state"]
    except Exception as e:
        print("Error:", e)
        return False

    if state == "success":
        return True
    else:
        return False
        
    return False

{%- endif %}    


{% if pipeline_settings.pull_fiscal_attributes -%}

def _fiscal_attr_handler(cur):
    result = cur.fetchall()
    return result

def _get_fiscal_attributes(**kwargs) -> None:
    """Gets fiscal attributes and add to overall execution parameter"""
    ti = kwargs["ti"]

    # Instantiate DBXHook
    hook = DatabricksSqlHook(kwargs["databricks_conn_id"]{%-if pipeline_settings.sql_endpoint_name -%}, sql_endpoint_name=kwargs["sql_endpoint_name"] {% endif %}{%-if pipeline_settings.sql_endpoint_http_path -%}, http_path=kwargs["sql_endpoint_http_path"] {% endif %})

    fiscal_attributes_sql = """
        SELECT 
            fiscal_yr, fiscal_yr_and_half_yr_desc, fiscal_yr_and_per_desc, fiscal_yr_and_qtr_desc, fiscal_yr_and_wk_desc, fiscal_wk_and_qtr_desc, SUBSTR(fiscal_wk_ending_date, 1, 10) AS fiscal_wk_ending_date, SUBSTR(DATE_SUB(fiscal_wk_ending_date, 6), 1, 10) As fiscal_wk_starting_date, SUBSTR(date_date, 1, 10) AS execution_date, SUBSTR(DATE_SUB(date_date, 1), 1, 10) AS prev_date
        FROM ids_coredata.dim_date
        WHERE date_date = '{ds}'
    """.format(**kwargs)

    response = hook.run(sql=fiscal_attributes_sql, handler=_fiscal_attr_handler)
    #schema = response[0]
    result_data = response[1][0]

    ti.xcom_push(key='fiscal_yr', value=result_data[0])
    ti.xcom_push(key='fiscal_yr_and_half_yr_desc', value=result_data[1])
    ti.xcom_push(key='fiscal_yr_and_per_desc', value=result_data[2])
    ti.xcom_push(key='fiscal_yr_and_qtr_desc', value=result_data[3])
    ti.xcom_push(key='fiscal_yr_and_wk_desc', value=result_data[4])
    ti.xcom_push(key='fiscal_wk_and_qtr_desc', value=result_data[5])
    ti.xcom_push(key='fiscal_wk_ending_date', value=result_data[6])
    ti.xcom_push(key='fiscal_wk_starting_date', value=result_data[7])
    ti.xcom_push(key='execution_date', value=result_data[8])
    ti.xcom_push(key='prev_date', value=result_data[9])

    {% if pipeline_settings.dynamic_attributes -%}
    for k, sql in {{pipeline_settings.dynamic_attributes}}.items():
        res = hook.run(sql=sql.replace('${', '{').format(**kwargs), handler=_fiscal_attr_handler)[1][0]
        ti.xcom_push(key=k, value=res[0])
    {%- endif %}

{%- endif %}    

# Create Global DAG
@dag(
    dag_id=DAG_NAME,
    start_date={{pipeline_settings.dag_start_date}},
    schedule_interval={{pipeline_settings.dag_schedule}},
    default_args=default_args,
    default_view="graph",
    max_active_runs={{pipeline_settings.max_active_runs|default(1)}},
    {% if pipeline_settings.tags %}
    tags={{pipeline_settings.tags}},
    {% endif %}
    **dag_args
)
def dag_main():

    {% if sensors -%}

    # Create task groups for sensors block
    with TaskGroup(group_id="Sensors") as sensors:

        {% for task in sensors%}

        {{task.task_id}}_args = {{task.arguments}}

        {% if task.operator == 'HttpSensor' %} # Generate HttpSensor task

        {{task.task_id}}_args['response_check'] = eval(str({{task.task_id}}_args['response_check']))
        {{task.task_id}}_args['timeout'] = eval(str({{task.task_id}}_args['timeout']))

        {{task.task_id}} = HttpSensor(
            task_id='{{task.task_id}}',
            on_success_callback=_task_on_success_callback,
            **{{task.task_id}}_args
        )

        {%- endif %}

        {% endfor %}   


    {%- endif %}

    {% if pipeline_settings.pull_fiscal_attributes -%}
    # Start operator to pull in fiscal attributes based on execution date
    begin = PythonOperator(
                task_id='begin',
                op_kwargs=default_args,
                python_callable=_get_fiscal_attributes,
                on_success_callback=_task_on_success_callback,
            )
    
    fiscal_parameters = {
        'fiscal_yr': {{ "\"{{ti.xcom_pull(key='fiscal_yr', task_ids='begin')}}\"" }},
        'fiscal_yr_and_half_yr_desc': {{ "\"{{ti.xcom_pull(key='fiscal_yr_and_half_yr_desc', task_ids='begin')}}\"" }},
        'fiscal_yr_and_per_desc': {{ "\"{{ti.xcom_pull(key='fiscal_yr_and_per_desc', task_ids='begin')}}\"" }},
        'fiscal_yr_and_qtr_desc': {{ "\"{{ti.xcom_pull(key='fiscal_yr_and_qtr_desc', task_ids='begin')}}\"" }},
        'fiscal_yr_and_wk_desc': {{ "\"{{ti.xcom_pull(key='fiscal_yr_and_wk_desc', task_ids='begin')}}\"" }},
        'fiscal_wk_and_qtr_desc': {{ "\"{{ti.xcom_pull(key='fiscal_wk_and_qtr_desc', task_ids='begin')}}\"" }},
        'fiscal_wk_ending_date': {{ "\"{{ti.xcom_pull(key='fiscal_wk_ending_date', task_ids='begin')}}\"" }},
        'fiscal_wk_starting_date': {{ "\"{{ti.xcom_pull(key='fiscal_wk_starting_date', task_ids='begin')}}\"" }},
        'execution_date': {{ "\"{{ti.xcom_pull(key='execution_date', task_ids='begin')}}\"" }},
        'prev_date': {{ "\"{{ti.xcom_pull(key='prev_date', task_ids='begin')}}\"" }},
        {% if pipeline_settings.dynamic_attributes -%}
        {% for key, value in pipeline_settings.dynamic_attributes.items() %}
        '{{key}}': {{ "\"{{ti.xcom_pull(key='"}}{{key}}{{"', task_ids='begin')}}\"" }},
        {%- endfor %}
        {%- endif %}
    }

    {% else %}
    # DummyOperator placeholder for begin task
    begin = DummyOperator(task_id="begin", on_success_callback=_task_on_success_callback,)
    {%- endif %}

    # Last task will only trigger if no previous task failed
    end = DummyOperator(task_id="end", trigger_rule=TriggerRule.NONE_FAILED, on_success_callback=on_success_callback)

    {% if pre_checks %} 
    # Create task groups for each pre_check block
    with TaskGroup(group_id="Pre-Checks") as pre_checks:
        pre_check_tasks_groups = [] # List of Pre check task groups in the TaskGroup
        {% for group in pre_checks %}
        with TaskGroup(group_id="{{group.group_name}}") as {{group.group_name}}:

            tasks = [] # list of tasks in group
            {% for task in group.tasks%}
            {{task.task_id}}_args = {{task.arguments}}

            {% if pipeline_settings.pull_fiscal_attributes -%}
            {{task.task_id}}_args['sql'] = {{task.task_id}}_args['sql'] if isinstance({{task.task_id}}_args['sql'], list) else [{{task.task_id}}_args['sql']]
            for k,v in {'${' + k + '}': v for k,v in fiscal_parameters.items()}.items(): {{task.task_id}}_args['sql'] = [sql.replace(k, v) for sql in {{task.task_id}}_args['sql']]
            {%- endif %}

            {{task.task_id}} = DatabricksSqlOperator(
                task_id='{{task.task_id}}',
                on_success_callback=_task_on_success_callback,
                **{{task.task_id}}_args
            )
            
            {{task.task_id}}_check = ShortCircuitOperator(
                task_id='{{task.task_id}}_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.{{group.group_name}}.{{task.task_id}}', 'sql_task_args': {{task.task_id}}_args},
                #ignore_downstream_trigger_rules=False,
                python_callable=_verify_check_response,
            )

            {% if group.is_sequential_execution -%}
            tasks.extend([{{task.task_id}}, {{task.task_id}}_check])
            {% else %}
            {{task.task_id}} >> {{task.task_id}}_check
            {% endif %}

            {% endfor %}

            {% if group.is_sequential_execution -%}
            # Set sequential execution for {{group.group_name}} elements
            chain(*tasks)
            {% endif %}        

        pre_check_tasks_groups.append({{group.group_name}})
        {% endfor %}
        chain(*pre_check_tasks_groups)
    {% endif %}

    {% if execs %} 
    # Create task groups for each exec block
    with TaskGroup(group_id="Execs") as execs:
        exec_tasks_groups = [] # List of Exec task groups in the TaskGroup
        {% for group in execs %}
        {{create_group(pipeline_settings, 'Execs', group, 8)}}
        {% endfor %}
        chain(*exec_tasks_groups)
    {% endif %}

    {% if post_checks %} 
    # Create task groups for each post_check block
    with TaskGroup(group_id="Post-Checks") as post_checks:
        post_check_tasks_groups = [] # List of Post check task groups in the TaskGroup
        {% for group in post_checks %}
        with TaskGroup(group_id="{{group.group_name}}") as {{group.group_name}}:

            tasks = [] # list of tasks in group
            {% for task in group.tasks %}
            {{task.task_id}}_args = {{task.arguments}}

            {% if pipeline_settings.pull_fiscal_attributes -%}
            {{task.task_id}}_args['sql'] = {{task.task_id}}_args['sql'] if isinstance({{task.task_id}}_args['sql'], list) else [{{task.task_id}}_args['sql']]
            for k,v in {'${' + k + '}': v for k,v in fiscal_parameters.items()}.items(): {{task.task_id}}_args['sql'] = [sql.replace(k, v) for sql in {{task.task_id}}_args['sql']]
            {%- endif %}

            {{task.task_id}} = DatabricksSqlOperator(
                task_id='{{task.task_id}}',
                on_success_callback=_task_on_success_callback,
                **{{task.task_id}}_args
            )
            
            {{task.task_id}}_check = ShortCircuitOperator(
                task_id='{{task.task_id}}_check',
                op_kwargs={'sql_task_id': 'Post-Checks.{{group.group_name}}.{{task.task_id}}'},
                python_callable=_verify_check_response,
            )

            {% if group.is_sequential_execution -%}
            tasks.extend([{{task.task_id}}, {{task.task_id}}_check])
            {% else %}
            {{task.task_id}} >> {{task.task_id}}_check
            {% endif %}

            {% endfor %}

            {% if group.is_sequential_execution -%}
            # Set sequential execution for {{group.group_name}} elements
            chain(*tasks)
            {% endif %}  

        post_check_tasks_groups.append({{group.group_name}})
        {% endfor %}
        chain(*post_check_tasks_groups)
    {% endif %}

    {% if data_quality %} 
    # Data Quality Computations
    with TaskGroup(group_id="Data-Quality") as data_quality:

        {%- for dq in data_quality %}
        
        {{dq.id}}_args = {'notebook_task': {'notebook_path': '{{pipeline_settings.dbx_base_path}}/{{pipeline_settings.dag_name}}_{{dq.id}}_metric'}{% if dq.cluster_args%}, **{{dq.cluster_args}} {% endif %}}
        {% if pipeline_settings.pull_fiscal_attributes -%}
        {{dq.id}}_args['notebook_task'] = {**{{dq.id}}_args['notebook_task'], **{'base_parameters':  fiscal_parameters}}
        {% endif %}

        {{dq.id}} = DatabricksSubmitRunOperator(
            task_id='{{dq.id}}',
            new_cluster={{dq.new_cluster}},
            on_success_callback=_task_on_success_callback,
            **{{dq.id}}_args
        )

        {%- endfor -%}
    {% endif %}

    # High-level dependencies between tasks
    chain({%if sensors -%} sensors,{% endif %} begin, {%if pre_checks -%} pre_checks,{% endif %} {%-if execs -%} execs, {% endif %} {%-if data_quality -%} data_quality, {% endif %} {%-if post_checks -%} post_checks, {% endif %}end)

# Instantiate main DAG
dag = dag_main()
