# 
# This file was programmatically generated by the DAG GENERATOR at https://git.corp.adobe.com/ccea/dag-dq-generator
#
# PLEASE DO NOT MANUALLY MODIFY THE FILE
# 

# General Imports
import os
from datetime import datetime, timedelta
from typing import List
import logging

# Airflow Imports
from airflow import DAG
from airflow.providers.databricks.hooks.databricks_sql import DatabricksSqlHook
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator, DatabricksRunNowOperator
from airflow.providers.databricks.operators.databricks_sql import DatabricksSqlOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup # Used to group tasks together in the Graph view of the Airflow UI
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.email import EmailOperator
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.utils.edgemodifier import Label # Used to label node edges in the Airflow UI
from airflow.decorators import dag, task # DAG and task decorators for interfacing with the TaskFlow API
from airflow.models.baseoperator import chain # A function that sets sequential dependencies between tasks including lists of tasks
from airflow.utils.trigger_rule import TriggerRule # Used to change how an Operator is triggered


# Define global constants
DAG_NAME = "demo-dev-01"
DAG_OWNER = "dmeci"
DATABRICKS_SQL_ENDPOINT_NAME = "airflowdbxsqlOperatorTesting"
DATABRICKS_CONN_ID = "shantipole"
# Instantiate Databricks cluster settings if any

NEW_TRANSCIENT_CLUSTER001 = {'spark_version': '9.1.x-scala2.12', 'policy_id': 'C96203490C00011D', 'spark_conf': {'spark.hadoop.fs.azure.account.oauth2.client.secret.ariaprime.dfs.core.windows.net': "{{ '{{secrets/ids_platform/data_secret_clientsecret}}' }}", 'spark.hadoop.fs.azure.account.oauth.provider.type.ariaprime.dfs.core.windows.net': 'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider', 'spark.hadoop.fs.azure.account.auth.type.ariaprime.dfs.core.windows.net': 'OAuth', 'spark.hadoop.fs.azure.account.oauth2.client.id.ariaprime.dfs.core.windows.net': "{{ '{{secrets/ids_platform/data_secret_clientid}}' }}", 'spark.hadoop.fs.azure.account.oauth2.client.endpoint.ariaprime.dfs.core.windows.net': 'https://login.microsoftonline.com/fa7b1b5a-7b34-4387-94ae-d2c178decee1/oauth2/token'}}

NEW_TRANSCIENT_CLUSTER002 = {'spark_version': '9.1.x-scala2.12', 'policy_id': 'C96203490C00011D', 'spark_conf': {'spark.hadoop.fs.azure.account.oauth2.client.secret.ariaprime.dfs.core.windows.net': "{{ '{{secrets/ids_platform/data_secret_clientsecret}}' }}", 'spark.hadoop.fs.azure.account.oauth2.client.endpoint.ariaprime.dfs.core.windows.net': 'https://login.microsoftonline.com/fa7b1b5a-7b34-4387-94ae-d2c178decee1/oauth2/token'}}

# Define arguments w/ extra arguments if necessary
extra_operator_args = {'depends_on_past': False, 'email': 'ccea-data-engineering@adobe.com', 'email_on_failure': True, 'email_on_retry': False}

# get the airflow.task logger
task_logger = logging.getLogger('airflow.task')

default_args = {
    "owner": DAG_OWNER,
    "databricks_conn_id": DATABRICKS_CONN_ID,
    
    "sql_endpoint_name": DATABRICKS_SQL_ENDPOINT_NAME,
    
    "do_xcom_push": True,
    
    **extra_operator_args,
    
}
dag_args = {'catchup': True}

# Define Python Operator callable functions
def _verify_check_response(**kwargs) -> None:
    """Pulls and returns DatabricksSqlOperator data from XCom"""
    ti = kwargs["ti"]
    pulled_value = ti.xcom_pull(key='return_value', task_ids=kwargs["sql_task_id"])

    return pulled_value[0][0]

    

# Create Global DAG
@dag(
    dag_id=DAG_NAME,
    start_date=datetime(2021, 6, 11),
    schedule_interval='@daily',
    default_args=default_args,
    default_view="graph",
    
    tags=['tag #1', 'tag #2', 'tag #3'],
    
    **dag_args
)
def dag_main():

    
    # DummyOperator placeholder for first task
    begin = DummyOperator(task_id="begin")

    # Last task will only trigger if no previous task failed
    end = DummyOperator(task_id="end", trigger_rule=TriggerRule.NONE_FAILED)

     
    # Create task groups for each pre_check block
    with TaskGroup(group_id="Pre-Checks") as pre_checks:
        pre_check_tasks_groups = [] # List of Pre check task groups in the TaskGroup
        
        with TaskGroup(group_id="pre_check_gp_01") as pre_check_gp_01:

            tasks = [] # list of tasks in group
            
            task_101_args = {'sql': ['drop table if exists default.my_airflow_table', 'create table default.my_airflow_table(id int, v string)', "insert into default.my_airflow_table values (1, 'test 1'), (2, 'test 2')"]}

            

            task_101 = DatabricksSqlOperator(
                task_id='task_101',
                **task_101_args
            )
            
            task_101_check = ShortCircuitOperator(
                task_id='task_101_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_01.task_101'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_101, task_101_check])
            

            
            task_102_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_102 = DatabricksSqlOperator(
                task_id='task_102',
                **task_102_args
            )
            
            task_102_check = ShortCircuitOperator(
                task_id='task_102_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_01.task_102'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_102, task_102_check])
            

            
            task_103_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_103 = DatabricksSqlOperator(
                task_id='task_103',
                **task_103_args
            )
            
            task_103_check = ShortCircuitOperator(
                task_id='task_103_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_01.task_103'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_103, task_103_check])
            

            
            task_104_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_104 = DatabricksSqlOperator(
                task_id='task_104',
                **task_104_args
            )
            
            task_104_check = ShortCircuitOperator(
                task_id='task_104_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_01.task_104'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_104, task_104_check])
            

            
            task_105_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_105 = DatabricksSqlOperator(
                task_id='task_105',
                **task_105_args
            )
            
            task_105_check = ShortCircuitOperator(
                task_id='task_105_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_01.task_105'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_105, task_105_check])
            

            

            # Set sequential execution for pre_check_gp_01 elements
            chain(*tasks)
                    

        pre_check_tasks_groups.append(pre_check_gp_01)
        
        with TaskGroup(group_id="pre_check_gp_02") as pre_check_gp_02:

            tasks = [] # list of tasks in group
            
            task_201_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_201 = DatabricksSqlOperator(
                task_id='task_201',
                **task_201_args
            )
            
            task_201_check = ShortCircuitOperator(
                task_id='task_201_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_02.task_201'},
                python_callable=_verify_check_response,
            )

            
            task_201 >> task_201_check
            

            
            task_202_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_202 = DatabricksSqlOperator(
                task_id='task_202',
                **task_202_args
            )
            
            task_202_check = ShortCircuitOperator(
                task_id='task_202_check',
                op_kwargs={'sql_task_id': 'Pre-Checks.pre_check_gp_02.task_202'},
                python_callable=_verify_check_response,
            )

            
            task_202 >> task_202_check
            

            

                    

        pre_check_tasks_groups.append(pre_check_gp_02)
        
        chain(*pre_check_tasks_groups)
    

     
    # Create task groups for each exec block
    with TaskGroup(group_id="Execs") as execs:
        exec_tasks_groups = [] # List of Exec task groups in the TaskGroup
        
        with TaskGroup(group_id="exec_gp_01") as exec_gp_01:

            tasks = [] # list of tasks in group
            
            task_101_args = {'sql': 'insert into table 01'}

             # Generate SQLOperator task
            

            task_101 = DatabricksSqlOperator(
                task_id='task_101',
                **task_101_args
            )
            
            
            
            tasks.append(task_101)

            
            task_102_args = {'job_id': 777, 'notebook_params': {'Variable': 5}}

             # Generate RunNowOperator task
            

            task_102 = DatabricksRunNowOperator(
                task_id='task_102',
                **task_102_args
            )

            
            
            tasks.append(task_102)

            
            task_103_args = {'job_id': 777, 'notebook_params': {'Variable': 5}}

             # Generate RunNowOperator task
            

            task_103 = DatabricksRunNowOperator(
                task_id='task_103',
                **task_103_args
            )

            
            
            tasks.append(task_103)

            

            # Set sequential execution for exec_gp_01 elements
            chain(*tasks)
            

        exec_tasks_groups.append(exec_gp_01)
        
        with TaskGroup(group_id="exec_gp_02") as exec_gp_02:

            tasks = [] # list of tasks in group
            
            task_201_args = {'notebook_task': {'notebook_path': '/Users/hnankam@adobe.com/testnotebook'}}

             # Generate SubmitNowOperator task
            

            task_201 = DatabricksSubmitRunOperator(
                task_id='task_201',
                new_cluster=NEW_TRANSCIENT_CLUSTER001,
                **task_201_args
            )

            
            
            tasks.append(task_201)

            
            task_202_args = {'trigger_dag_id': 'example-dag-basic', 'wait_for_completion': True}

             # Generate TriggerDagRunOperator task

            task_202 = TriggerDagRunOperator(
                task_id='task_202',
                **task_202_args
            )

            
            
            tasks.append(task_202)

            

            # Set sequential execution for exec_gp_02 elements
            chain(*tasks)
            

        exec_tasks_groups.append(exec_gp_02)
        
        chain(*exec_tasks_groups)
    

     
    # Create task groups for each post_check block
    with TaskGroup(group_id="Post-Checks") as post_checks:
        post_check_tasks_groups = [] # List of Post check task groups in the TaskGroup
        
        with TaskGroup(group_id="post_check_gp_01") as post_check_gp_01:

            tasks = [] # list of tasks in group
            
            task_101_args = {'sql': ['drop table if exists default.my_airflow_table', 'create table default.my_airflow_table(id int, v string)', "insert into default.my_airflow_table values (1, 'test 1'), (2, 'test 2')"]}

            

            task_101 = DatabricksSqlOperator(
                task_id='task_101',
                **task_101_args
            )
            
            task_101_check = ShortCircuitOperator(
                task_id='task_101_check',
                op_kwargs={'sql_task_id': 'Post-Checks.post_check_gp_01.task_101'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_101, task_101_check])
            

            
            task_102_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_102 = DatabricksSqlOperator(
                task_id='task_102',
                **task_102_args
            )
            
            task_102_check = ShortCircuitOperator(
                task_id='task_102_check',
                op_kwargs={'sql_task_id': 'Post-Checks.post_check_gp_01.task_102'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_102, task_102_check])
            

            
            task_103_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_103 = DatabricksSqlOperator(
                task_id='task_103',
                **task_103_args
            )
            
            task_103_check = ShortCircuitOperator(
                task_id='task_103_check',
                op_kwargs={'sql_task_id': 'Post-Checks.post_check_gp_01.task_103'},
                python_callable=_verify_check_response,
            )

            tasks.extend([task_103, task_103_check])
            

            

            # Set sequential execution for post_check_gp_01 elements
            chain(*tasks)
              

        post_check_tasks_groups.append(post_check_gp_01)
        
        with TaskGroup(group_id="post_check_gp_02") as post_check_gp_02:

            tasks = [] # list of tasks in group
            
            task_201_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_201 = DatabricksSqlOperator(
                task_id='task_201',
                **task_201_args
            )
            
            task_201_check = ShortCircuitOperator(
                task_id='task_201_check',
                op_kwargs={'sql_task_id': 'Post-Checks.post_check_gp_02.task_201'},
                python_callable=_verify_check_response,
            )

            
            task_201 >> task_201_check
            

            
            task_202_args = {'sql': 'select * from default.my_airflow_table'}

            

            task_202 = DatabricksSqlOperator(
                task_id='task_202',
                **task_202_args
            )
            
            task_202_check = ShortCircuitOperator(
                task_id='task_202_check',
                op_kwargs={'sql_task_id': 'Post-Checks.post_check_gp_02.task_202'},
                python_callable=_verify_check_response,
            )

            
            task_202 >> task_202_check
            

            

              

        post_check_tasks_groups.append(post_check_gp_02)
        
        chain(*post_check_tasks_groups)
    

     
    # Data Quality Computations
    with TaskGroup(group_id="Data-Quality") as data_quality:
        
        hau_activeuse_args = {'notebook_task': {'notebook_path': '/dbxworkspace/hau//sql/demo-dev-01_hau_activeuse_dq_metric'}}
        

        hau_activeuse = DatabricksSubmitRunOperator(
            task_id='hau_activeuse',
            new_cluster=NEW_TRANSCIENT_CLUSTER001,
            **hau_activeuse_args
        )
        
        trial_member_subs_activity_agg_args = {'notebook_task': {'notebook_path': '/dbxworkspace/hau//sql/demo-dev-01_trial_member_subs_activity_agg_dq_metric'}}
        

        trial_member_subs_activity_agg = DatabricksSubmitRunOperator(
            task_id='trial_member_subs_activity_agg',
            new_cluster=NEW_TRANSCIENT_CLUSTER001,
            **trial_member_subs_activity_agg_args
        )
        
        hybrid_user_activity_args = {'notebook_task': {'notebook_path': '/dbxworkspace/hau//sql/demo-dev-01_hybrid_user_activity_dq_metric'}}
        

        hybrid_user_activity = DatabricksSubmitRunOperator(
            task_id='hybrid_user_activity',
            new_cluster=NEW_TRANSCIENT_CLUSTER001,
            **hybrid_user_activity_args
        )

    # High-level dependencies between tasks
    chain(begin, pre_checks,execs, post_checks, data_quality, end)

# Instantiate main DAG
dag = dag_main()