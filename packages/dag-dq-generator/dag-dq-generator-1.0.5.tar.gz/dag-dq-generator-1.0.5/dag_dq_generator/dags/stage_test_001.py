# 
# This file was programmatically generated by the DAG GENERATOR at https://git.corp.adobe.com/ccea/dag-dq-generator
#
# PLEASE DO NOT MANUALLY MODIFY THE FILE
# 

# General Imports
import os
from datetime import datetime, timedelta
from typing import List
import logging

# Airflow Imports
from airflow import DAG
from airflow.providers.databricks.hooks.databricks_sql import DatabricksSqlHook
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator, DatabricksRunNowOperator
from airflow.providers.databricks.operators.databricks_sql import DatabricksSqlOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup # Used to group tasks together in the Graph view of the Airflow UI
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.email import EmailOperator
from airflow.operators.python import PythonOperator, ShortCircuitOperator
from airflow.utils.edgemodifier import Label # Used to label node edges in the Airflow UI
from airflow.decorators import dag, task # DAG and task decorators for interfacing with the TaskFlow API
from airflow.models.baseoperator import chain # A function that sets sequential dependencies between tasks including lists of tasks
from airflow.utils.trigger_rule import TriggerRule # Used to change how an Operator is triggered


# Define global constants
DAG_NAME = "stage_test_001"
DAG_OWNER = "dmecistg"

DATABRICKS_CONN_ID = "shantipolesql"
# Instantiate Databricks cluster settings if any

NEW_TRANSCIENT_CLUSTER001 = {'spark_version': '9.1.x-scala2.12', 'policy_id': 'C96203490C0007C5', 'spark_conf': {'spark.hadoop.fs.azure.account.oauth2.client.secret.agamar.dfs.core.windows.net': "{{'{{secrets/DBX-CCEA-READWRITE/dmeci_clientsecret}}'}}", 'spark.hadoop.fs.azure.account.oauth2.client.endpoint.agamar.dfs.core.windows.net': 'https://login.microsoftonline.com/fa7b1b5a-7b34-4387-94ae-d2c178decee1/oauth2/token', 'spark.hadoop.fs.azure.account.oauth2.client.secret.amethiaprime.dfs.core.windows.net': "{{'{{secrets/DBX-CCEA-READWRITE/dmeci_clientsecret}}'}}", 'spark.hadoop.fs.azure.account.oauth2.client.id.agamar.dfs.core.windows.net': "{{'{{secrets/DBX-CCEA-READWRITE/dmeci_clientid}}'}}", 'spark.hadoop.fs.azure.account.oauth.provider.type.agamar.dfs.core.windows.net': 'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider', 'spark.hadoop.fs.azure.account.oauth2.client.endpoint.amethiaprime.dfs.core.windows.net': 'https://login.microsoftonline.com/fa7b1b5a-7b34-4387-94ae-d2c178decee1/oauth2/token', 'spark.hadoop.fs.azure.account.auth.type.agamar.dfs.core.windows.net': 'OAuth', 'spark.hadoop.fs.azure.account.oauth2.client.id.amethiaprime.dfs.core.windows.net': "{{'{{secrets/DBX-CCEA-READWRITE/dmeci_clientid}}'}}", 'spark.hadoop.fs.azure.account.auth.type.amethiaprime.dfs.core.windows.net': 'OAuth', 'spark.hadoop.fs.azure.account.oauth.provider.type.amethiaprime.dfs.core.windows.net': 'org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider'}}

# Define arguments w/ extra arguments if necessary
extra_operator_args = {'depends_on_past': False, 'email': 'ccea-data-engineering@adobe.com', 'email_on_failure': True, 'email_on_retry': False}

# get the airflow.task logger
task_logger = logging.getLogger('airflow.task')

default_args = {
    "owner": DAG_OWNER,
    "databricks_conn_id": DATABRICKS_CONN_ID,
    
    "do_xcom_push": True,
    
    **extra_operator_args,
    
}
dag_args = {'catchup': True}

# Define Python Operator callable functions
def _verify_check_response(**kwargs) -> None:
    """Pulls and returns DatabricksSqlOperator data from XCom"""
    ti = kwargs["ti"]
    pulled_value = ti.xcom_pull(key='return_value', task_ids=kwargs["sql_task_id"])

    return pulled_value[0][0]

def _get_fiscal_attributes(**kwargs) -> None:
    """Gets fiscal attributes and add to overall execution parameter"""
    ti = kwargs["ti"]

    _date = ti.execution_date.strftime('%Y-%m-%d')

    # Instantiate DBXHook
    hook = DatabricksSqlHook(kwargs["databricks_conn_id"])

    fiscal_attributes_sql = """
        SELECT 
            fiscal_yr, fiscal_yr_and_half_yr_desc, fiscal_yr_and_per_desc, fiscal_yr_and_qtr_desc, fiscal_yr_and_wk_desc, fiscal_wk_and_qtr_desc, SUBSTR(fiscal_wk_ending_date, 1, 10)
        FROM ccea_stage.hn_ccmusage_dim_date
        WHERE date_date = '{_date}'
    """.format(_date=_date)

    response = hook.run(sql=fiscal_attributes_sql)
    #schema = response[0]
    result_data = response[1][0]

    ti.xcom_push(key='fiscal_yr', value=result_data[0])
    ti.xcom_push(key='fiscal_yr_and_half_yr_desc', value=result_data[1])
    ti.xcom_push(key='fiscal_yr_and_per_desc', value=result_data[2])
    ti.xcom_push(key='fiscal_yr_and_qtr_desc', value=result_data[3])
    ti.xcom_push(key='fiscal_yr_and_wk_desc', value=result_data[4])
    ti.xcom_push(key='fiscal_wk_and_qtr_desc', value=result_data[5])
    ti.xcom_push(key='fiscal_wk_ending_date', value=result_data[6])    

# Create Global DAG
@dag(
    dag_id=DAG_NAME,
    start_date=datetime(2022, 6, 11),
    schedule_interval=None,
    default_args=default_args,
    default_view="graph",
    
    tags=['tag #1', 'tag #2', 'tag #3'],
    
    **dag_args
)
def dag_main():

    # Start operator to pull in fiscal attributes based on execution date
    begin = PythonOperator(
                task_id='begin',
                op_kwargs=default_args,
                python_callable=_get_fiscal_attributes,
            )
    
    fiscal_parameters = {
        'fiscal_yr': "{{ti.xcom_pull(key='fiscal_yr', task_ids='begin')}}",
        'fiscal_yr_and_half_yr_desc': "{{ti.xcom_pull(key='fiscal_yr_and_half_yr_desc', task_ids='begin')}}",
        'fiscal_yr_and_per_desc': "{{ti.xcom_pull(key='fiscal_yr_and_per_desc', task_ids='begin')}}",
        'fiscal_yr_and_qtr_desc': "{{ti.xcom_pull(key='fiscal_yr_and_qtr_desc', task_ids='begin')}}",
        'fiscal_yr_and_wk_desc': "{{ti.xcom_pull(key='fiscal_yr_and_wk_desc', task_ids='begin')}}",
        'fiscal_wk_and_qtr_desc': "{{ti.xcom_pull(key='fiscal_wk_and_qtr_desc', task_ids='begin')}}",
        'fiscal_wk_ending_date': "{{ti.xcom_pull(key='fiscal_wk_ending_date', task_ids='begin')}}",
    }

    

    # Last task will only trigger if no previous task failed
    end = DummyOperator(task_id="end", trigger_rule=TriggerRule.NONE_FAILED)

    

     
    # Create task groups for each exec block
    with TaskGroup(group_id="Execs") as execs:
        exec_tasks_groups = [] # List of Exec task groups in the TaskGroup
        
        with TaskGroup(group_id="compute_trials_to_paid") as compute_trials_to_paid:

            tasks = [] # list of tasks in group
            
            compute_model_args = {'sql': 'SELECT COUNT(*) FROM ccea_stage.hn_ccmusage_dim_date WHERE date_date = "${fiscal_wk_ending_date}" AND fiscal_yr="${fiscal_yr}"'}

             # Generate SQLOperator task
            for k,v in {'${' + k + '}': v for k,v in fiscal_parameters.items()}.items(): compute_model_args['sql'] = compute_model_args['sql'].replace(k, v)

            compute_model = DatabricksSqlOperator(
                task_id='compute_model',
                **compute_model_args
            )
            
            
            
            tasks.append(compute_model)

            
            compute_activities_args = {'trigger_dag_id': 'kbohra_test_sqloperator', 'wait_for_completion': True}

             # Generate TriggerDagRunOperator task

            compute_activities = TriggerDagRunOperator(
                task_id='compute_activities',
                **compute_activities_args
            )

            
            
            tasks.append(compute_activities)

            

            # Set sequential execution for compute_trials_to_paid elements
            chain(*tasks)
            

        exec_tasks_groups.append(compute_trials_to_paid)
        
        chain(*exec_tasks_groups)
    

    

     
    # Data Quality Computations
    with TaskGroup(group_id="Data-Quality") as data_quality:
        
        hau_activeuse_args = {'notebook_task': {'notebook_path': '/dbxworkspace/hau/sql/stage_test_001_hau_activeuse_dq_metric'}}
        hau_activeuse_args['notebook_task'] = {**hau_activeuse_args['notebook_task'], **{'base_parameters':  fiscal_parameters}}
        

        hau_activeuse = DatabricksSubmitRunOperator(
            task_id='hau_activeuse',
            new_cluster=NEW_TRANSCIENT_CLUSTER001,
            **hau_activeuse_args
        )

    # High-level dependencies between tasks
    chain(begin, execs, data_quality, end)

# Instantiate main DAG
dag = dag_main()